{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yy0dLLDks0xu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Implementing Softmax Function\n",
        "# Softmax is a function that converts raw scores (logits) into probabilities.\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Preventing overflow by subtracting max value\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)  # Normalizing to sum up to 1"
      ],
      "metadata": {
        "id": "cDYlckFitj9t"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Prediction Function\n",
        "# This function predicts the class label with the highest probability\n",
        "def predict_softmax(X, W, b):\n",
        "    logits = np.dot(X, W) + b  # Computing raw scores\n",
        "    probabilities = softmax(logits)  # Converting the scores to probabilities\n",
        "    return np.argmax(probabilities, axis=1)  # Returning the class with highest probability"
      ],
      "metadata": {
        "id": "Jr_WQpDQtnLM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Loss and Cost Functions\n",
        "# Loss function measures how far off our predictions are from actual labels.\n",
        "def loss_softmax(y_pred, y):\n",
        "    return -np.sum(y * np.log(y_pred + 1e-9)) / y.shape[0]  # Cross-entropy loss\n",
        "\n",
        "# Cost function calculates the average loss for all data points.\n",
        "def cost_softmax(X, y, W, b):\n",
        "    logits = np.dot(X, W) + b\n",
        "    y_pred = softmax(logits)\n",
        "    return loss_softmax(y_pred, y)"
      ],
      "metadata": {
        "id": "2fnbAKdBtwLf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Computing Gradients\n",
        "# Computes the gradient (direction of steepest descent) for weight and bias updates.\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    logits = np.dot(X, W) + b\n",
        "    y_pred = softmax(logits)\n",
        "    grad_W = np.dot(X.T, (y_pred - y)) / X.shape[0]  # Gradient for weights\n",
        "    grad_b = np.sum(y_pred - y, axis=0) / X.shape[0]  # Gradient for biases\n",
        "    return grad_W, grad_b"
      ],
      "metadata": {
        "id": "z7uNWulUtwV9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: Gradient Descent Algorithm\n",
        "# Optimizing the weights and biases by adjusting them in the direction of the gradient.\n",
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter, show_cost=False):\n",
        "    cost_history = []\n",
        "    for i in range(n_iter):\n",
        "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "        W -= alpha * grad_W  # Updating weights\n",
        "        b -= alpha * grad_b  # Updating biases\n",
        "        cost = cost_softmax(X, y, W, b)  # Computing cost\n",
        "        cost_history.append(cost)\n",
        "        if show_cost and i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Cost: {cost:.4f}\")  # Printing the cost every 100 iterations\n",
        "    return W, b, cost_history"
      ],
      "metadata": {
        "id": "UuaXPoSNtxHd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6: Loading and Preparing the MNIST Dataset\n",
        "# This function loads the dataset, normalizes it, and splits it into training and testing sets.\n",
        "def load_and_prepare_mnist(csv_file, test_size=0.2, random_state=42):\n",
        "    df = pd.read_csv(csv_file)  # Reading the dataset\n",
        "    y = df.iloc[:, 0].values  # First column contains labels\n",
        "    X = df.iloc[:, 1:].values / 255.0  # Normalizing the pixel values (0-255 â†’ 0-1)\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=random_state)"
      ],
      "metadata": {
        "id": "I-ro8o5ft9PS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 7: Evaluating the Model\n",
        "# Measuring how well the model performs using accuracy metrics.\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    return cm, precision, recall, f1"
      ],
      "metadata": {
        "id": "-uUpjld4t9fH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMVPYPYUcr9O",
        "outputId": "6006ccbc-3568-4481-e977-3cf910ff3a3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the MNIST dataset\n",
        "X_train, X_test, y_train, y_test = load_and_prepare_mnist('/content/drive/MyDrive/AI-ML/mnist_dataset.csv')"
      ],
      "metadata": {
        "id": "6UB9MpgluESz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 8: One-hot Encoding Labels\n",
        "# Converting the numerical labels into binary vectors.\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = encoder.transform(y_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "8BfyHqYOuEWN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 9: Model Initialization\n",
        "# Initializing the weights and biases with small random values.\n",
        "num_features = X_train.shape[1]  # Number of input features (pixels)\n",
        "num_classes = y_train.shape[1]  # Number of classes (digits 0-9)\n",
        "W = np.random.randn(num_features, num_classes) * 0.01  # Small random weights\n",
        "b = np.zeros(num_classes)  # Bias initializing to zeros"
      ],
      "metadata": {
        "id": "JW-jCaofuEZf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 10: Training the Model\n",
        "alpha = 0.1\n",
        "n_iter = 1000\n",
        "W_opt, b_opt, cost_history = gradient_descent_softmax(X_train, y_train, W, b, alpha, n_iter, show_cost=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agWn4YxnuEhZ",
        "outputId": "8ac39ae2-dc43-4cd7-df78-cb288e1fa05e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 2.2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 11: Plotting Cost Function Convergence\n",
        "# Visualizing how the cost function decreases over time.\n",
        "plt.plot(cost_history)\n",
        "plt.title('Cost Function vs. Iterations')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bQ-ED11NuWgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 12: Model Evaluation\n",
        "# Using the trained model to predict on the test set.\n",
        "y_pred_test = predict_softmax(X_test, W_opt, b_opt)\n",
        "y_test_labels = np.argmax(y_test, axis=1)  # Converting the labels back to numbers\n",
        "cm, precision, recall, f1 = evaluate_classification(y_test_labels, y_pred_test)"
      ],
      "metadata": {
        "id": "C0xWJW3XuWuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the evaluation metrics\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "P_nhw3XbuXV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 13: Visualizing Confusion Matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cax = ax.imshow(cm, cmap='copper_r', interpolation='nearest')\n",
        "ax.set_xticks(np.arange(num_classes))\n",
        "ax.set_yticks(np.arange(num_classes))\n",
        "ax.set_xticklabels([str(i) for i in range(num_classes)], fontsize=10)\n",
        "ax.set_yticklabels([str(i) for i in range(num_classes)], fontsize=10)\n",
        "plt.colorbar(cax)\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Actual Label', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Adding the numbers to each cell in the confusion matrix\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, cm[i, j], ha='center', va='center', color='yellow' if cm[i, j] > cm.max()/2 else 'red', fontsize=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I25ZlfVgwDyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiT5QPrM4yIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}